<!DOCTYPE html>
<html lang="es">

<head>
            <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">


        <title>Despliegue de un cluster de Kubernetes | Javier Pérez Hidalgo</title>

        <!-- Bootstrap Core CSS -->
        <link href="/theme/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="/theme/css/clean-blog.min.css" rel="stylesheet">

        <!-- Code highlight color scheme -->
            <link href="/theme/css/code_blocks/darkly.css" rel="stylesheet">


        <!-- Custom Fonts -->
        <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->



        <meta name="description" content="En este artículo vamos a crear un cluster de Kubernetes (k8s) y para ello he decidido utilizar la distribución k3s. Posteriormente,...">

        <meta name="author" content="Javier Pérez Hidalgo">

        <meta name="tags" content="Kubernetes">
        <meta name="tags" content="Let's Chat">

	                <meta property="og:locale" content="">
		<meta property="og:site_name" content="Javier Pérez Hidalgo">

	<meta property="og:type" content="article">
            <meta property="article:author" content="/author/javier-perez-hidalgo.html">
	<meta property="og:url" content="/despliegue-de-un-cluster-de-kubernetes.html">
	<meta property="og:title" content="Despliegue de un cluster de Kubernetes">
	<meta property="article:published_time" content="2018-03-05 00:00:00+01:00">
            <meta property="og:description" content="En este artículo vamos a crear un cluster de Kubernetes (k8s) y para ello he decidido utilizar la distribución k3s. Posteriormente,...">

            <meta property="og:image" content="theme/images/banner-kubernetes.jpg">
</head>

<body class="article-despliegue-de-un-cluster-de-kubernetes">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
        <!--    <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>-->
                <a class="navbar-brand" href="/">Inicio</a>
                <a class="navbar-brand" href="/categories">Categorías</a>
                <a class="navbar-brand" href="/authors">Sobre mí</a>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
        <header class="intro-header" style="background-image: url('/theme/images/banner-kubernetes.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>Despliegue de un cluster de Kubernetes</h1>
                        <span class="meta">Publicado por
                                <a href="/author/javier-perez-hidalgo.html">Javier Pérez Hidalgo</a>
                             el lun 05 marzo 2018
                        </span>
                        
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
    <!-- Post Content -->
    <article>
        <p>En este artículo vamos a crear un <strong>cluster de Kubernetes (k8s)</strong> y para ello he decidido utilizar la distribución <strong>k3s</strong>. Posteriormente, desplegaremos la aplicación <strong>Let's Chat</strong> en él.</p>
<p>Todo el proceso se llevará a cabo en varias instancias de <em>OpenStack</em>:</p>
<ul>
<li><strong>controlador:</strong> máquina que controlará el <em>cluster</em>. Posee la dirección IP 172.22.201.59</li>
<li><strong>worker1:</strong> máquina que actuará como <em>worker</em>. Posee la dirección IP 172.22.201.69</li>
<li><strong>worker2:</strong> máquina que actuará como <em>worker</em>. Posee la dirección IP 172.22.201.79</li>
</ul>
<h2>¿Qué es k3s?</h2>
<p><strong>k3s</strong> es una distribución de <em>Kubernetes</em>, desarrollada por Rancher Labs, muy ligera y muy fácil de instalar, que requiere pocos requisitos y un uso de memoria mínimo.</p>
<p>Para el planteamiento de un entorno de desarrollo, esto se convierte en una gran mejora sobre lo que hemos hablado anteriormente en <em>Kubernetes</em>; crear un entorno mínimo para desarrollo, donde la creación del entorno es compleja y requiere de muchos recursos, aunque sea <em>Ansible</em> el que realice el trabajo difícil.</p>
<p>Entre las herramientas que nos proporciona se incluye <strong>kubectl</strong>. Esta herramienta, es una interfaz de línea de comandos desarrollada en <em>Go</em> para gestionar nuestros <em>clusters</em> de manera centralizada.</p>
<p>A continuación podemos ver un diagrama acerca de la estructura interna de <em>k3s</em>:</p>
<p><img alt="." src="images/hlc_despliegue_de_un_cluster_de_kubernetes/estructurainterna.png"></p>
<h2>Instalación de k3s en el controlador</h2>
<p>Para llevar a cabo la instalación del <em>software</em> de <strong>k3s</strong>, vamos a utilizar el <em>script</em> de instalación que se nos proporciona. Para ello, necesitaremos la herramienta <code>curl</code> en nuestro sistema, así que vamos a instalarla:</p>
<pre>
apt install curl -y
</pre>

<p>Una vez instalada, procederemos a la descarga del propio <em>software</em> ejecutando el siguiente comando:</p>
<pre>
root@controlador:~# curl -sfL https://get.k3s.io | sh -
[INFO]  Finding release for channel stable
[INFO]  Using v1.20.4+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Creating /usr/local/bin/ctr symlink to k3s
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
</pre>

<p>Realizada la instalación, ya dispondríamos de todas las herramientas necesarias, incluyendo <code>kubectl</code>. Para comprobarlo vamos a listar los nodos existentes en el <em>cluster</em>:</p>
<pre>
root@controlador:~# kubectl get nodes
NAME          STATUS   ROLES                  AGE    VERSION
controlador   Ready    control-plane,master   113s   v1.20.4+k3s1
</pre>

<p>Lógicamente tan sólo nos muestra uno, que hace referencia al propio nodo que acabamos de instalar, ya que aún no hemos asociado ningún <em>worker</em>.</p>
<p>Es el momento de vincular los <em>workers</em>, para ello, necesitaremos conocer el <em>token</em> del nodo maestro. Para conocer dicho <em>token</em> ejecutamos el siguiente comando:</p>
<pre>
root@controlador:~# cat /var/lib/rancher/k3s/server/node-token
K10fad848963eda043b3b917043618456893554b621a76f832e41883708a4dc094a::server:af342b2794ad1f7bcdfe0a7bd0ae9f31
</pre>

<p>Hecho esto, es el momento de pasar con la instalación de <em>k3s</em> en los <em>workers</em>.</p>
<h2>Instalación de k3s en los workers</h2>
<p>Para llevar a cabo la instalación del <em>software</em> de <strong>k3s</strong> en estas máquinas, volveremos a utilizar el <em>script</em> de instalación que se nos proporciona, pero esta vez, tendremos que indicarle dos parámetros para llevar a cabo la vinculación al nodo maestro. Dichos parámetros son:</p>
<ul>
<li><strong>K3S_URL:</strong> indica la URL del controlador, a la que se conectará el <em>worker</em>.</li>
<li><strong>K3S_TOKEN:</strong> indica el <em>token</em> del nodo maestro.</li>
</ul>
<p>De igual manera, volveremos a necesitar la herramienta <code>curl</code> en nuestro sistema, así que vamos a instalarla:</p>
<pre>
apt install curl -y
</pre>

<p>Llevamos a cabo las instalaciones:</p>
<p><strong>worker1</strong></p>
<pre>
root@worker1:~# curl -sfL https://get.k3s.io | K3S_URL=https://172.22.201.59:6443 K3S_TOKEN=K10fad848963eda043b3b917043618456893554b621a76f832e41883708a4dc094a::server:af342b2794ad1f7bcdfe0a7bd0ae9f31 sh -
[INFO]  Finding release for channel stable
[INFO]  Using v1.20.4+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Creating /usr/local/bin/ctr symlink to k3s
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service
[INFO]  systemd: Enabling k3s-agent unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.
[INFO]  systemd: Starting k3s-agent
</pre>

<p><strong>worker2</strong></p>
<pre>
root@worker2:~# curl -sfL https://get.k3s.io | K3S_URL=https://172.22.201.59:6443 K3S_TOKEN=K10fad848963eda043b3b917043618456893554b621a76f832e41883708a4dc094a::server:af342b2794ad1f7bcdfe0a7bd0ae9f31 sh -
[INFO]  Finding release for channel stable
[INFO]  Using v1.20.4+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.20.4+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Creating /usr/local/bin/ctr symlink to k3s
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service
[INFO]  systemd: Enabling k3s-agent unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.
[INFO]  systemd: Starting k3s-agent
</pre>

<p>Finalizada la instalación en ambos <em>workers</em>, vamos a listar los nodos existentes en el nodo maestro:</p>
<pre>
root@controlador:~# kubectl get nodes
NAME          STATUS   ROLES                  AGE     VERSION
controlador   Ready    control-plane,master   8m25s   v1.20.4+k3s1
worker1       Ready    <none>                 81s     v1.20.4+k3s1
worker2       Ready    <none>                 51s     v1.20.4+k3s1
</pre>

<p>Podemos ver como efectivamente, ahora sí nos muestra los dos <em>workers</em> que acabamos de vincular.</p>
<h2>Conectando nuestro cluster a la máquina anfitriona</h2>
<p>Una vez tenemos nuestro <em>cluster</em> listo, vamos a configurarlo para que en vez de gestionarlo desde la máquina <em>controlador</em>, lo podamos gestionar desde nuestra máquina anfitriona, lo cuál sería mucho más cómodo.</p>
<p>Lo primero que debemos hacer, es instalar <code>kubectl</code> en nuestra máquina anfitriona. Para hacer esto, debemos ejecutar los siguientes comandos:</p>
<pre>
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

apt update && apt install kubectl -y
</pre>

<p>Una vez disponemos de la herramienta, vamos a crear el directorio <code>~/.kube</code>:</p>
<pre>
root@debian:~# mkdir .kube
</pre>

<p>¿Y para qué creamos este directorio? Pues porque para conectarnos a nuestro <em>cluster</em> remotamente, necesitaremos un fichero llamado <code>config</code> almacenado en esta ruta.</p>
<p>Bien, el contenido del fichero <code>config</code> debe ser el mismo contenido que podemos encontrar en el fichero <code>/etc/rancher/k3s/k3s.yaml</code> del nodo maestro de nuestro <em>cluster</em>, por tanto copiamos dicho contenido. En mi caso queda de esta manera:</p>
<pre>
root@debian:~# cat .kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUyTVRRNU5qZ3pPREl3SGhjTk1qRXdNekExTVRneE9UUXlXaGNOTXpFd016QXpNVGd4T1RReQpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUyTVRRNU5qZ3pPREl3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFTL254RkNkRk1kODlPQ1VObTJTa3h0cWk3WUlGUGFBVDR2ckRzY2tJYTkKVzNha3l1di9MNVVlakdXK1FrWVF1TXp5a1pXdGVOKytIQSt4SkZ5YkZPdW9vMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXlWM1lYS2Q2dWNYNDVhajY1Nk9mCktJZ2RnYmt3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQUpBVTNDekpBME9RSGR1SnUyZzNSeXBjQUZnVHFyem8KbWZaMUhtN0YvWUZEQWlCbzhpSEdsUDZvUXJKYUI4Q2lKUnJWSVJQNkFWeGJzQTBjZlBsalBSMG9BZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrakNDQVRlZ0F3SUJBZ0lJVWdQTGJDWXFFMWN3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOakUwT1RZNE16Z3lNQjRYRFRJeE1ETXdOVEU0TVRrME1sb1hEVEl5TURNdwpOVEU0TVRrME1sb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJKeDRyUkFsRWdpN0RQZGQKTTJ4NGFBQVVEdEdBSzR2Rzlhd2ZmL1RsZ0YxZ1NaRHZRMTBwKzd0WHg3RjdPajZoQVNGcWNWNlZxaEtidU53ZwpyT2ZLRFZXalNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVEpTRE9YUnFZQzZuMWtZOHJkUHQ0VWo3VllDVEFLQmdncWhrak9QUVFEQWdOSkFEQkcKQWlFQTllUzhpUGpoaHloVWZYL01nZXJqOFN3dmlNMnUzNzFPanMycXVkanJNQlVDSVFDRG5qSmQzbFpoMVphZwpjUmcwdU1QMzdaQ0lTTnBtQWVDMHRjYXlCT3hOTHc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCi0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlCZURDQ0FSMmdBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwClpXNTBMV05oUURFMk1UUTVOamd6T0RJd0hoY05NakV3TXpBMU1UZ3hPVFF5V2hjTk16RXdNekF6TVRneE9UUXkKV2pBak1TRXdId1lEVlFRRERCaHJNM010WTJ4cFpXNTBMV05oUURFMk1UUTVOamd6T0RJd1dUQVRCZ2NxaGtqTwpQUUlCQmdncWhrak9QUU1CQndOQ0FBVFBhR0FxbFhkRk5LSW96VDFjZUhXVEFxTXlGczFCV0JnbjB6dDNnd0FnClEzUVJUb0QrRzJZQi84WTl0SDZQQzAzbktRYW1PakNjV3BlSkRxdFQzVzVBbzBJd1FEQU9CZ05WSFE4QkFmOEUKQkFNQ0FxUXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVV5VWd6bDBhbUF1cDlaR1BLM1Q3ZQpGSSsxV0Frd0NnWUlLb1pJemowRUF3SURTUUF3UmdJaEFNTGs3TDNhQk9Nc0kwWURRQVhkREs4bkpzNDZLR0pPCm50alRYcUNLL2JSRUFpRUF3QnowZll3S1BaY0toV0hhYUw2S1IydlFmTnhFTjNNZUZUS0l4VEViMXBRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUVaWDF5WkNMRDBEbExac3dZSWcvY1lSNysxYlMyTHJyb1JwMXRGVkJheGpvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFbkhpdEVDVVNDTHNNOTEwemJIaG9BQlFPMFlBcmk4YjFyQjkvOU9XQVhXQkprTzlEWFNuNwp1MWZIc1hzNlBxRUJJV3B4WHBXcUVwdTQzQ0NzNThvTlZRPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
</pre>

<p>Una vez tenemos el mismo contenido en nuestro equipo anfitrión, debemos cambiar el valor del parámetro <code>server</code>, y en él establecer la dirección del nodo maestro del <em>cluster</em>. En mi caso esta directiva quedaría tal que así:</p>
<pre>
root@debian:~# cat .kube/config
...
    server: https://172.22.201.59:6443
...
</pre>

<p>En teoría ya habríamos realizado toda la configuración y podríamos gestionar nuestro <em>cluster</em> remotamente, para ello vamos a intentar los nodos existentes en él mediante el siguiente comando: <code>kubectl get nodes</code>. En este punto, en mi caso, me reportó un error debido a certificados x509, que logré solucionar con el siguiente comando:</p>
<pre>
kubectl --insecure-skip-tls-verify cluster-info dump
</pre>

<p>Tras él, volvemos a intentar listar los nodos:</p>
<pre>
root@debian:~# kubectl get nodes
NAME          STATUS     ROLES                  AGE   VERSION
worker1       Ready      <none>                 30m   v1.20.4+k3s1
worker2       Ready      <none>                 30m   v1.20.4+k3s1
controlador   Ready      control-plane,master   37m   v1.20.4+k3s1
</pre>

<p>Efectivamente podemos ver los tanto el nodo maestro como los <em>workers</em> por lo que ya podríamos gestionar nuestro <em>cluster</em> de manera remota.</p>
<h2>Desplegando una aplicación en nuestro cluster</h2>
<p>Para desplegar la aplicación <strong>Let's Chat</strong> utilizaremos <a href="https://github.com/iesgn/kubernetes-storm">este repositorio</a> de <em>GitHub</em>, que contiene todos los ficheros <code>.yaml</code> en los que se definen los <em>deployment</em>, los servicios, ...</p>
<p>En primer lugar, lógicamente clonaremos dicho repositorio. Si no disponemos de la herramienta <code>git</code>, tendremos que instalarla.</p>
<pre>
root@debian:/home/javier/Kubernetes# git clone https://github.com/iesgn/kubernetes-storm.git
Clonando en 'kubernetes-storm'...
remote: Enumerating objects: 288, done.
remote: Counting objects: 100% (288/288), done.
remote: Compressing objects: 100% (213/213), done.
remote: Total 288 (delta 119), reused 224 (delta 60), pack-reused 0
Recibiendo objetos: 100% (288/288), 6.36 MiB | 8.71 MiB/s, listo.
Resolviendo deltas: 100% (119/119), listo.
</pre>

<p>Cuando hayamos clonado el repositorio, tendremos que dirigirnos a la ruta <code>unidad3/ejemplos-3.2/ejemplo8/</code> que es donde se encuentran los ficheros de esta aplicación:</p>
<pre>
root@debian:/home/javier/Kubernetes# cd kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8/

root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# ls -l
total 20
-rw-r--r-- 1 root root 247 mar  5 20:15 ingress.yaml
-rw-r--r-- 1 root root 394 mar  5 20:15 letschat-deployment.yaml
-rw-r--r-- 1 root root 177 mar  5 20:15 letschat-srv.yaml
-rw-r--r-- 1 root root 358 mar  5 20:15 mongo-deployment.yaml
-rw-r--r-- 1 root root 149 mar  5 20:15 mongo-srv.yaml
</pre>

<p>Podemos observar que hay cinco ficheros. Dos de ellos definen los <strong>deployment</strong> para la base de datos <strong>MongoDB</strong> y para la propia aplicación <strong>Let's Chat</strong>. Otros dos nos ofrecerán los <strong>servicios</strong> de dichos procesos. El último fichero <code>ingress.yaml</code> lo veremos más adelante.</p>
<p>En este punto, todo estaría listo para definir el primer <em>deployment</em>, en este caso el de <em>MongoDB</em>. Este <em>deployment</em> tendrá como consecuencia la generación de un <strong>ReplicaSet</strong> con un <strong>pod</strong>, que ejecutará una imagen <em>mongo</em>.</p>
<p>Para definir dicho <em>deployment</em> utilizaremos el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl apply -f mongo-deployment.yaml
deployment.apps/mongo created
</pre>

<p>Podemos apreciar en la salida del comando como efectivamente se ha definido dicho <em>deployment</em>.</p>
<p>El siguiente paso, será definir el <em>servicio</em> de nuestra base de datos, esto nos permitirá poder acceder a ella. Ejecutamos el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl apply -f mongo-srv.yaml
service/mongo created
</pre>

<p>Una vez hayamos creado el <em>servicio</em>, habremos terminado con lo relativo a <em>MongoDB</em>.</p>
<p>El mismo proceso que hemos llevado a cabo con nuestra base de datos, tendremos que seguir con nuestra aplicación.</p>
<p>Por tanto, empezaremos por definir su <em>deployment</em>, que al igual que el anterior, generará un <strong>ReplicaSet</strong> con sólo un <strong>pod</strong>, que ejecutará una imagen <em>sdelements/lets-chat</em>.</p>
<p>Definiremos el <em>deployment</em> utilizando el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl apply -f letschat-deployment.yaml
deployment.apps/letschat created
</pre>

<p>Podemos apreciar en la salida del comando como efectivamente se ha definido este <em>deployment</em>.</p>
<p>El siguiente paso, será definir el <em>servicio</em> de nuestra aplicación, esto nos permitirá poder acceder a ella. Ejecutamos el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl apply -f letschat-srv.yaml
service/letschat created
</pre>

<p>Una vez hayamos creado el <em>servicio</em>, habremos terminado con lo relativo a <em>Let's Chat</em>, y por tanto todo estaría preparado.</p>
<p>Para comprobar que los <em>deployment</em> han sido correctamente creados, vamos a utilizar este comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl get deploy,rs,po -o wide
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                 SELECTOR
deployment.apps/mongo      1/1     1            1           34m   mongo        mongo                  name=mongo
deployment.apps/letschat   1/1     1            1           30m   letschat     sdelements/lets-chat   name=letschat

NAME                                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                 SELECTOR
replicaset.apps/mongo-5c694c878b      1         1         1       34m   mongo        mongo                  name=mongo,pod-template-hash=5c694c878b
replicaset.apps/letschat-7c66bd64f5   1         1         1       30m   letschat     sdelements/lets-chat   name=letschat,pod-template-hash=7c66bd64f5

NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
pod/mongo-5c694c878b-bwhsr      1/1     Running   0          34m    10.42.1.3   worker1   <none>           <none>
pod/letschat-7c66bd64f5-467dp   1/1     Running   0          105s   10.42.2.3   worker2   <none>           <none>
</pre>

<p>Y para comprobar que los <em>servicios</em> han sido correctamente creados, vamos a utilizar este comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.43.0.1      <none>        443/TCP          96m
mongo        ClusterIP   10.43.108.67   <none>        27017/TCP        8m14s
letschat     NodePort    10.43.5.244    <none>        8080:32094/TCP   111s
</pre>

<p>En este momento, ya tendríamos disponible nuestra aplicación y podríamos acceder a ella. Para ello nos dirigiremos a nuestro navegador e introduciremos la dirección IP del nodo maestro, pero además de esto, debemos indicar el puerto donde se está sirviendo <em>Let's Chat</em>. Para conocer este puerto, que por defecto nos lo asigna en el rango comprendido entre 30000 y 40000, podemos utilizar el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl describe service/letschat | grep NodePort
Type:                     NodePort
NodePort:                 http  32094/TCP
</pre>

<p>Podemos ver como, en mi caso, está utilizando el puerto 32094, por tanto yo introduciré en mi navegador la dirección <code>172.22.201.59:32094</code>:</p>
<p><img alt="." src="images/hlc_despliegue_de_un_cluster_de_kubernetes/letschat.png"></p>
<p>¡Vaya! Aquí podemos ver como efectivamente poseemos nuestra aplicación.</p>
<p>Pero, ¿no es demasiado incómodo tener que introducir este puerto tan inusual cada vez que queramos acceder a la aplicación? Para solucionar esto, pasaremos al siguiente apartado.</p>
<h2>Proxy inverso con Ingress</h2>
<p>Gracias a los controladores <strong>Ingress</strong> o <strong>Ingress controller</strong> podemos realizar un <em>proxy inverso</em>, y así evitar tener que acceder a direcciones tan incómodas como la anterior, indicando nuestras aplicaciones por medio de nombres.</p>
<p>Si recordamos, poseemos un fichero llamado <code>ingress.yaml</code> que aún no hemos utilizado. Al definir este fichero, cambiaremos el comportamiento y podremos acceder a nuestra aplicación en la dirección <code>www.letschat.com</code>, usando resolución estática claro. Ejecutamos el siguiente comando:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl apply -f ingress.yaml
Warning: networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.networking.k8s.io/ingress-letschat created
</pre>

<p>Parece que se ha generado el nuevo <em>ingress</em>, pero vamos a comprobarlo listando los <em>ingress</em> existentes en nuestro <em>cluster</em>:</p>
<pre>
root@debian:/home/javier/Kubernetes/kubernetes-storm/unidad3/ejemplos-3.2/ejemplo8# kubectl get ingress
NAME               CLASS    HOSTS              ADDRESS     PORTS   AGE
ingress-letschat   <none>   www.letschat.com   10.0.0.13   80      30s
</pre>

<p>Podemos apreciar como efectivamente se ha creado el <em>ingress</em> y ahora está utilizando el puerto 80.</p>
<p>Por último, añadiremos la línea relativa al nodo maestro en nuestro fichero <code>/etc/hosts</code>. En mi caso, añado la siguiente línea:</p>
<pre>
172.22.201.59   www.letschat.com
</pre>

<p>Hecho esto, nos dirigimos a nuestro navegador e introducimos la dirección <code>www.letschat.com</code>:</p>
<p><img alt="." src="images/hlc_despliegue_de_un_cluster_de_kubernetes/letschat80.png"></p>
<p>¡Bien! Ahora podremos acceder a <em>Let's Chat</em> siempre que queramos en la dirección <code>www.letschat.com</code>.</p>
<p>.</p>
    </article>

        <div class="tags">
            <p><strong><a href="/tags">tags:</a></strong> <a href="/tag/kubernetes.html">Kubernetes</a>, <a href="/tag/lets-chat.html">Let&#39;s Chat</a></p>
        </div>

    <hr>

            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                            <li>
                                <a href="https://www.instagram.com/javierpzh/">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-instagram fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://twitter.com/jperezhid_">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/javierpzh">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://www.facebook.com/javier.perezhidalgo.904">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="mailto:javierperezhidalgo01@gmail.com">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-envelope-square fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                    </ul>
<p class="copyright text-muted">
    Blog creado por <a href="http://www.instagram.com/javierpzh/">Javier Pérez Hidalgo</a>,
    con la utilización de <a href="https://blog.getpelican.com/">Pelican</a>. <br />        &copy;  Javier Pérez Hidalgo
</p>                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="/theme/js/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="/theme/js/bootstrap.min.js"></script>

        <!-- Custom Theme JavaScript -->
        <script src="/theme/js/clean-blog.min.js"></script>

</body>

</html>